{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path=\"D:/LEX/ProjectS-master/DATA/All60Corpus/\"\n",
    "doclist=os.listdir(path)\n",
    "corpus=[]\n",
    "wfile=open('All_60.txt',\"a\",encoding='utf-8')\n",
    "for i in doclist:\n",
    "    file=open(path+i,\"r\",encoding='utf-8')\n",
    "    try:\n",
    "        s=str(file.read())\n",
    "    except:\n",
    "        print(i)\n",
    "    corpus.append(s)\n",
    "    wfile.write(s+'\\n')\n",
    "    file.close()\n",
    "#print(corpus)\n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~“”'''\n",
    "stop_words = stopwords.words('english') + list(punctuations)\n",
    " \n",
    "def tokenize(text):\n",
    "    for c in punctuations:\n",
    "             text= text.replace(c,\" \")\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for doc in corpus:\n",
    "    words = tokenize(doc)\n",
    "    vocabulary.update(words)\n",
    "    \n",
    "vocabulary=list(vocabulary)\n",
    "word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    " \n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "DOCUMENTS_COUNT = len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf = defaultdict(lambda: 0.0)\n",
    "for doc in corpus:\n",
    "    words = set(tokenize(doc))\n",
    "    for word in words:\n",
    "        word_idf[word] += 1\n",
    "for word in vocabulary:\n",
    "    word_idf[word] =float(math.log(DOCUMENTS_COUNT / (1+word_idf[word]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tf(word, document):\n",
    "    return float(document.count(word)) / len(document)\n",
    " \n",
    "def tf_idf(word, document):\n",
    "    \n",
    "    document=open(path+document,\"r\",encoding='utf-8')\n",
    "    document = tokenize(document.read())\n",
    " \n",
    "    if word not in word_index:\n",
    "        return .0\n",
    "    Tf=word_tf(word, document)\n",
    "    Idf=word_idf[word]\n",
    "    return Tf*Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"D:/LEX/ProjectS-master/DATA/Neutral/neutral_negative.txt\",\"r\",encoding='utf-8')\n",
    "neg_words=file.read().split()\n",
    "file=open(\"D:/LEX/ProjectS-master/DATA/Neutral/positive_neutral.txt\",\"r\",encoding='utf-8')\n",
    "pos_words=file.read().split()\n",
    "pos_score,neg_score={},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words=['protest']\n",
    "neg_score={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list=[]\n",
    "for p in pos_words:\n",
    "    score,d=0.0,0\n",
    "    for l in doclist:\n",
    "        tscore=tf_idf(p,l)\n",
    "        if tscore>0.0:\n",
    "            list.append(tscore)\n",
    "            d+=1\n",
    "            score=+tscore\n",
    "    try:\n",
    "        pos_score[p]=max(list)\n",
    "        #pos_score[p]=score/d\n",
    "    except:\n",
    "        print(p)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "for n in neg_words:\n",
    "    score,d=0.0,0\n",
    "    for l in doclist:\n",
    "        tscore=tf_idf(n,l)\n",
    "        if tscore>0.0:\n",
    "            list.append(tscore)\n",
    "            d+=1\n",
    "            score=+tscore\n",
    "    try:\n",
    "        neg_score[n]=max(list)\n",
    "        #neg_score[n]=score/d\n",
    "    except:\n",
    "        print(n)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achievement': 0.06054126524684952,\n",
       " 'durable': 0.05772215271281936,\n",
       " 'empowered': 0.07422158219252518,\n",
       " 'expanded': 0.07422158219252518,\n",
       " 'expansion': 0.06054126524684952,\n",
       " 'fastest': 0.07422158219252518,\n",
       " 'gainfully': 0.07422158219252518,\n",
       " 'grow': 0.06054126524684952,\n",
       " 'growing': 0.05772215271281936,\n",
       " 'growth': 0.07422158219252518,\n",
       " 'imminent': 0.06054126524684952,\n",
       " 'importantly': 0.02637120662821608,\n",
       " 'impressed': 0.05772215271281936,\n",
       " 'increase': 0.06054126524684952,\n",
       " 'increased': 0.06054126524684952,\n",
       " 'increasing': 0.02637120662821608,\n",
       " 'incredible': 0.07422158219252518,\n",
       " 'incremental': 0.05772215271281936,\n",
       " 'largely': 0.02637120662821608,\n",
       " 'organised': 0.06054126524684952,\n",
       " 'pace': 0.07422158219252518,\n",
       " 'profits': 0.06054126524684952,\n",
       " 'raise': 0.06054126524684952,\n",
       " 'reconciliation': 0.05772215271281936,\n",
       " 'recovery': 0.05772215271281936,\n",
       " 'relaxation': 0.07422158219252518,\n",
       " 'resiliency': 0.07422158219252518,\n",
       " 'simplified': 0.05772215271281936,\n",
       " 'simplify': 0.02637120662821608,\n",
       " 'smoother': 0.05772215271281936,\n",
       " 'structured': 0.05772215271281936,\n",
       " 'uplifting': 0.06054126524684952,\n",
       " 'upside': 0.05772215271281936}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'protest': 0.03582397526189089}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3329"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_words.index('wasted')+1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key in pos_score:\n",
    "    line=lines[lex_words.index(key)]\n",
    "    print(line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
