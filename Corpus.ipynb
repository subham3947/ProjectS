{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path=\"E:/ProjectS-master/DATA/All15Corpus/\"\n",
    "doclist=os.listdir(path)\n",
    "corpus=[]\n",
    "wfile=open('All_15.txt',\"a\",encoding='utf-8')\n",
    "for i in doclist:\n",
    "    file=open(path+i,\"r\",encoding='utf-8')\n",
    "    s=str(file.read())\n",
    "    corpus.append(s)\n",
    "    wfile.write(s+'\\n')\n",
    "    file.close()\n",
    "#print(corpus)\n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~“”'''\n",
    "stop_words = stopwords.words('english') + list(punctuations)\n",
    " \n",
    "def tokenize(text):\n",
    "    for c in punctuations:\n",
    "             text= text.replace(c,\" \")\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for doc in corpus:\n",
    "    words = tokenize(doc)\n",
    "    vocabulary.update(words)\n",
    "    \n",
    "vocabulary=list(vocabulary)\n",
    "word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    " \n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "DOCUMENTS_COUNT = len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idf = defaultdict(lambda: 0.0)\n",
    "for doc in corpus:\n",
    "    words = set(tokenize(doc))\n",
    "    for word in words:\n",
    "        word_idf[word] += 1\n",
    "for word in vocabulary:\n",
    "    word_idf[word] =float(math.log(DOCUMENTS_COUNT / (1+word_idf[word]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_tf(word, document):\n",
    "    return float(document.count(word)) / len(document)\n",
    " \n",
    "def tf_idf(word, document):\n",
    "    \n",
    "    document=open(path+document,\"r\",encoding='utf-8')\n",
    "    document = tokenize(document.read())\n",
    " \n",
    "    if word not in word_index:\n",
    "        return .0\n",
    "    Tf=word_tf(word, document)\n",
    "    Idf=word_idf[word]\n",
    "    return Tf*Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file=open(\"E:/ProjectS-master/DATA/Neutral/Negative-Set.txt\",\"r\",encoding='utf-8')\n",
    "neg_words=file.read().split()\n",
    "file=open(\"E:/ProjectS-master/DATA/Neutral/Positive-Set.txt\",\"r\",encoding='utf-8')\n",
    "pos_words=file.read().split()\n",
    "pos_score,neg_score={},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evolution\n",
      "user-friendly\n"
     ]
    }
   ],
   "source": [
    "for p in pos_words:\n",
    "    score,d=0.0,0\n",
    "    for l in doclist:\n",
    "        tscore=tf_idf(p,l)\n",
    "        if tscore>0.0:\n",
    "            d+=1\n",
    "            score=+tscore\n",
    "    try:\n",
    "        pos_score[p]=score/d\n",
    "    except:\n",
    "        print(p)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spiked\n",
      "death\n"
     ]
    }
   ],
   "source": [
    "for n in neg_words:\n",
    "    score,d=0.0,0\n",
    "    for l in doclist:\n",
    "        tscore=tf_idf(n,l)\n",
    "        if tscore>0.0:\n",
    "            d+=1\n",
    "            score=+tscore\n",
    "    try:\n",
    "        neg_score[n]=score/d\n",
    "    except:\n",
    "        print(n)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adulteration': 0.034028438762728666,\n",
       " 'attack': 0.0051603566475346765,\n",
       " 'bankruptcy': 0.012769224527227353,\n",
       " 'barrier': 0.006595399647832241,\n",
       " 'brake': 0.011127783292077146,\n",
       " 'burden': 0.0006008200813773249,\n",
       " 'burdened': 0.009885851246885693,\n",
       " 'compromising': 0.016771159104487697,\n",
       " 'corruption': 0.001696845405937946,\n",
       " 'crisis': 0.0004905364422422316,\n",
       " 'dark': 0.09030624133185684,\n",
       " 'defect': 0.009564001118648788,\n",
       " 'delay': 0.0003473928140976471,\n",
       " 'dented': 0.03452885697982761,\n",
       " 'derail': 0.02433121528112205,\n",
       " 'devil': 0.003460047936409993,\n",
       " 'dip': 0.003108251494557895,\n",
       " 'disappointment': 0.007247397704642553,\n",
       " 'discourage': 0.0004905364422422316,\n",
       " 'erosion': 0.049956644141027184,\n",
       " 'hiccup': 0.010071501035559604,\n",
       " 'inflation': 0.014364103021861339,\n",
       " 'killing': 0.02257656033296421,\n",
       " 'lag': 0.004544978221555499,\n",
       " 'lagging': 0.01156631662378462,\n",
       " 'loss': 0.0004073356483914066,\n",
       " 'losses': 0.00043699988396498924,\n",
       " 'malpractice': 0.005143400382537301,\n",
       " 'npa': 0.0036145403884990657,\n",
       " 'obstruction': 0.03640251588570973,\n",
       " 'plunge': 0.02043075924356377,\n",
       " 'pollution': 0.00325528030436145,\n",
       " 'problem': 0.00036118223589746084,\n",
       " 'recession': 0.028288702103955156,\n",
       " 'restriction': 0.024457940360711226,\n",
       " 'rubbish': 0.03378362985076659,\n",
       " 'scared': 0.0017264428489913808,\n",
       " 'scattered': 0.016891572602946422,\n",
       " 'shut': 0.0005043278818337346,\n",
       " 'slowdown': 0.0024290705767239075,\n",
       " 'stagnation': 0.009207695194620696,\n",
       " 'suicide': 0.01707608927002384,\n",
       " 'tension': 0.0032976998239161205,\n",
       " 'thwarting': 0.014230074391686533,\n",
       " 'wasted': 0.015396473931988707}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lex_words=[]\n",
    "f = open(\"improvedsamplelexicon.txt\",\"r\",encoding='utf-8')\n",
    "lines=f.readlines()\n",
    "for line in lines:\n",
    "    \n",
    "    w=line.split('\\t')[1]\n",
    "    \n",
    "    lex_words.append(w)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3329"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_words.index('wasted')+1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key in pos_score:\n",
    "    line=lines[lex_words.index(key)]\n",
    "    print(line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
