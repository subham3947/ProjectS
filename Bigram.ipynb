{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\tag\\stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "model='stanford-postagger-2018-02-27/models/english-left3words-distsim.tagger'\n",
    "jar='stanford-postagger-2018-02-27/stanford-postagger-3.9.1.jar'\n",
    "st=StanfordPOSTagger(model,jar,encoding='utf-8')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_words = []\n",
    "f=open(\"D:/Lex/SentiWordNet_3.0.0_20130122.txt\",\"r\",encoding=\"utf-8\") \n",
    "all_lines=f.readlines()\n",
    "for line in all_lines:\n",
    "    \n",
    "    w=line.split('\\t')[4]\n",
    "    #print(w)\n",
    "    i=w.index('#')\n",
    "    #print(i)\n",
    "    senti_words.append(w[:i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_words = []\n",
    "f=open(\"improvedsamplelexicon.txt\",\"r\",encoding=\"utf-8\") \n",
    "all_lines=f.readlines()\n",
    "for line in all_lines:\n",
    "    \n",
    "    w=line.split('\\t')[1]\n",
    "    senti_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3795"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_words.index('hoax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg= {\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\",\"no\" \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\"minimise"\",\"unfulfilled\",\"undue\",\"anti\",\"against\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost={\"absolutely\":0.5, \"amazingly\":0.125, \"awfully\":0.25, \"completely\":0.25, \"considerably\":0.125,\n",
    " \"decidedly\":0.25 , \"deeply\":0.25 , \"enormously\":0.25 ,\n",
    " \"entirely\":0.5 , \"especially\":0.25 , \"exceptionally\":0.25, \"extremely\":0.625 ,\"fabulously\":0.25   , \"fully\":0.375  ,\n",
    " \"greatly\":0.125 , \"highly\":0.5   ,\"huge\":0.25, \"hugely\":0.25   , \"incredibly\":0.25   ,\n",
    " \"intensely\":0.25   , \"majorly\":0.625  ,\"particularly\":0.125 ,\"really\":0.375 ,\"substantially\":0.125   ,\n",
    " \"thoroughly\":0.625   , \"totally\":0.5  ,\"unbelievably\":0.25   , \"utterly\":0.5   ,\n",
    " \"very\":0.25  ,\"tremendously\":-0.25 ,  \"barely\":-0.375   ,\"badly\":-0.125 ,\"hardly\":-0.25,\"unusually\":-0.5   , \n",
    " \"less\":-0.5   , \"little\":-0.375   , \"marginally\":-0.125   , \"partly\":-0.125   ,\n",
    " \"scarcely\":-0.25   , \"slightly\":-0.25   , \"somewhat\":-0.125, \"shoddy\":-0.625, \"poorly\":-0.75}\n",
    "pos_boost=[\"absolutely\", \"amazingly\", \"awfully\", \"completely\", \"considerably\",\n",
    " \"decidedly\",\"deeply\",\"enormously\",\"entirely\",\"especially\",\"exceptionally\",\"extremely\",\"fabulously\",\"fully\",\n",
    " \"greatly\",\"highly\",\"huge\",\"hugely\", \"incredibly\",\"intensely\",\"majorly\",\"particularly\",\"really\",\"substantially\",\"thoroughly\",\"totally\",\"unbelievably\", \"utterly\",\"very\"]\n",
    "neg_boost=[\"tremendously\", \"barely\",\"badly\",\"hardly\",\"unusually\", \"less\", \"little\", \"marginally\", \"partly\",\n",
    " \"scarcely\", \"slightly\",\"somewhat\", \"shoddy\", \"poorly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(word,pos):\n",
    "    found=0\n",
    "    line_nos=[index for index, value in enumerate(senti_words) if value == word]\n",
    "    lines=[all_lines[lno] for lno in line_nos]\n",
    "    if len(lines)==0:\n",
    "        return None\n",
    "    for l in lines:\n",
    "        if l[0]==pos:\n",
    "            pos_score,neg_score=l.split('\\t')[2],l.split('\\t')[3]\n",
    "            found=1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    if found==1:\n",
    "        return(float(pos_score)-float(neg_score))\n",
    "    else:\n",
    "        pos_score,neg_score=lines[0].split('\\t')[2],lines[0].split('\\t')[3]\n",
    "        return(float(pos_score)-float(neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word,pos):\n",
    "    second_sent=find_sentiment(word,pos)\n",
    "            \n",
    "    if second_sent is None:\n",
    "        try:\n",
    "            lemma=lemmatizer.lemmatize(word,pos)\n",
    "            synsets = wn.synsets(lemma)\n",
    "            synset=synsets[0]\n",
    "            s=synset.name()\n",
    "            i=s.index('.')\n",
    "            second_sent=find_sentiment(s[:i],pos)\n",
    "            #print(s[:i],second_sent)\n",
    "            if second_sent is None:\n",
    "                second_sent=0.0\n",
    "        except:\n",
    "            second_sent=0.0\n",
    "    return second_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(score):\n",
    "    if score>=0.0:\n",
    "        return((score-(0.0))/1.5)\n",
    "    else:\n",
    "        return(-(-score-(0.03582397526189089))/1.839176024738109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns=open('sent_nouns.txt','r',encoding='utf-8')\n",
    "nouns=nouns.read()\n",
    "nouns=nouns.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!@'#-+=_'''\n",
    "def clean_tweet(t):\n",
    "    for c in punctuations:\n",
    "             t= t.replace(c,\" \")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_booster(booster,word,pos2):\n",
    "    sec_score=get_sentiment(word,pos2)\n",
    "    if sec_score>0.0:\n",
    "        return(boost[booster]+sec_score)\n",
    "    if sec_score<0.0:\n",
    "        return(sec_score-boost[booster])\n",
    "    else:\n",
    "        return(boost[booster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_booster(booster,word,pos2):\n",
    "    sec_score=get_sentiment(word,pos2)\n",
    "    if sec_score>0.0:\n",
    "        return(sec_score+boost[booster])\n",
    "    if sec_score<0.0:\n",
    "        return(sec_score+boost[booster])\n",
    "    else:\n",
    "        return(boost[booster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1='''The marginalization of the Tax Cheats which was expected post #GST will now happen in right earnest as #Ewaybill rolls out. Real benefits to most organized sector companies will be seen in FY2019'''\n",
    "tweet2='''From Failed #Demonetisation to Shoddy Implementation of #GST to poorly performing #Economy to unfulfilled Promise of reduction in fuel prices to not keeping the promises of providing 2 crores jobs, BJP has wrecked a nation & has broken peopleâ€™s faith '''\n",
    "tweet3='''There is nothing wrong with #GST implementation but the fact is we are all losers. Injected to us by @INCIndia  Even God cannot change'''\n",
    "tweet4='''So, Indian businesses are mad against GST cause now they have to pay tax and earlier they weren't?'''\n",
    "tweet5='''#GST in india is total failure,it like nightmare for tax payers Indians..in my view it could be rolled out after #2019LSPolls after corrections ~ Dr Subramanian @Swamy39 Speaking at 14th Annual ðŸ‡®ðŸ‡³India Business Conference on â€˜Indian Growth @Columbia_Biz '''\n",
    "tweet6='''Ãˆven in high income countries life insurance & health insurance premiums don't attract #GSTForCommonMan.Then why torment us with #GST ? '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=word_tokenize(clean_tweet(tweet6.lower()))\n",
    "bigrams=nltk.ngrams(A,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ã¨ven', 'NN'), ('in', 'IN')]\n",
      "[('in', 'IN'), ('high', 'JJ')]\n",
      "Normal: [('in', 'IN'), ('high', 'JJ')] 0.125\n",
      "[('high', 'JJ'), ('income', 'NN')]\n",
      "[('income', 'NN'), ('countries', 'NNS')]\n",
      "[('countries', 'NNS'), ('life', 'NN')]\n",
      "[('life', 'NN'), ('insurance', 'NN')]\n",
      "[('insurance', 'NN'), ('&', 'CC')]\n",
      "[('&', 'CC'), ('health', 'NN')]\n",
      "[('health', 'NN'), ('insurance', 'NN')]\n",
      "[('insurance', 'NN'), ('premiums', 'NNS')]\n",
      "[('premiums', 'NNS'), ('don', 'VBP')]\n",
      "[('don', 'VB'), ('t', 'NN')]\n",
      "[('t', 'NN'), ('attract', 'VB')]\n",
      "[('attract', 'VB'), ('gstforcommonman.then', 'NN')]\n",
      "[('gstforcommonman.then', 'NN'), ('why', 'WRB')]\n",
      "[('why', 'WRB'), ('torment', 'VB')]\n",
      "Normal: [('why', 'WRB'), ('torment', 'VB')] 0.0\n",
      "[('torment', 'VB'), ('us', 'PRP')]\n",
      "[('us', 'PRP'), ('with', 'IN')]\n",
      "[('with', 'IN'), ('gst', 'NN')]\n",
      "[('gst', 'NN'), ('?', '.')]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "not_found=set()\n",
    "score=0.0\n",
    "for b in bigrams:\n",
    "    i+=1\n",
    "    b_tag=st.tag(b)\n",
    "    print(b_tag)\n",
    "    second_gram=b_tag[1]\n",
    "    first_gram=b_tag[0]\n",
    "    if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN') or second_gram[1].startswith('VBD'):\n",
    "    #if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VB'):\n",
    "        pos2=convert_tag(second_gram[1])\n",
    "        pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "        if first_gram[0] in boost:\n",
    "            if first_gram[0] in pos_boost:\n",
    "                score=positive_booster(first_gram[0],second_gram[0],pos2)\n",
    "                print('Pos-Boost:',b_tag,score)\n",
    "            if first_gram[0] in neg_boost:\n",
    "                score=negative_booster(first_gram[0],second_gram[0],pos2)\n",
    "                print('Neg-Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "        if first_gram[0] in neg:\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    score+=(get_sentiment(first_gram[0],pos1)+sec_score)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                print('Neg:',b_tag,score)\n",
    "            \n",
    "        elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "            try:\n",
    "                first_score=get_sentiment(first_gram[0],pos1)\n",
    "                score+=first_score+get_sentiment(second_gram[0],pos2)\n",
    "                print('Normal:',b_tag,score)\n",
    "            except:\n",
    "                print(\"Not Found:\",second_gram[0])\n",
    "                not_found.add(second_gram[0])\n",
    "    \n",
    "print(normalize(score))\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(bigrams):\n",
    "    score=0.0\n",
    "    for b in bigrams:\n",
    "        b_tag=st.tag(b)\n",
    "        #b_tag=pos_tag(b)\n",
    "        #print(b_tag)\n",
    "        second_gram=b_tag[1]\n",
    "        first_gram=b_tag[0]\n",
    "        if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN') or second_gram[1].startswith('VBD'):\n",
    "            pos2=convert_tag(second_gram[1])\n",
    "            pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "            if first_gram[0] in boost:\n",
    "                if first_gram[0] in pos_boost:\n",
    "                    score=positive_booster(first_gram[0],second_gram[0],pos2)\n",
    "                    print('Pos-Boost:',b_tag,score)\n",
    "                if first_gram[0] in neg_boost:\n",
    "                    score=negative_booster(first_gram[0],second_gram[0],pos2)\n",
    "                    print('Neg-Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "            if first_gram[0] in neg:\n",
    "                x=0.0\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    try:\n",
    "                        x=get_sentiment(first_gram[0],pos1)\n",
    "                        score+=(x+sec_score)\n",
    "                    except:\n",
    "                        print(x)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                #print('Neg:',b_tag,score)\n",
    "            \n",
    "            elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "                try:\n",
    "                    first_score=get_sentiment(first_gram[0],pos1)\n",
    "                    score+=first_score+get_sentiment(second_gram[0],pos2)\n",
    "                    #print('Normal:',b_tag,score)\n",
    "                except:\n",
    "                    print(\"Not Found:\",second_gram[0])\n",
    "                    #not_found.add(second_gram[0])\n",
    "    return (normalize(score))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "Neg-Boost: [('badly', 'RB'), ('affected', 'VBN')] -0.625\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "afford 0.0\n",
      "23\n",
      "Pos-Boost: [('very', 'RB'), ('good', 'JJ')] 0.875\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "Pos-Boost: [('really', 'RB'), ('killing', 'VBG')] -0.625\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "going 0.0\n",
      "49\n",
      "50\n",
      "Pos-Boost: [('completely', 'RB'), ('stopped', 'VBD')] 0.25\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "brought 0.0\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "giving 0.0\n",
      "profiteering 0.0\n",
      "88\n",
      "advantage 0.625\n",
      "paying 0.0\n",
      "89\n",
      "90\n",
      "91\n",
      "fair 0.375\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "Neg-Boost: [('little', 'JJ'), ('demonetisation', 'NN')] -0.875\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "Pos-Boost: [('huge', 'JJ'), ('discomfort', 'NN')] -0.375\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "Neg-Boost: [('little', 'RB'), ('hard', 'JJ')] -1.125\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "accurate 0.5\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "path=\"D:/LEX/ProjectS-master/tweet/\"\n",
    "tweet_list=os.listdir(path)\n",
    "fw=open(\"tweets.txt\",\"r\",encoding=\"utf-8\")\n",
    "rw=open(\"stan_tweet_result.txt\",\"a\",encoding='utf-8')\n",
    "#tw=fw.read().split('\\n')\n",
    "for i in tweet_list:\n",
    "    score=0.0\n",
    "    file=open(path+i,\"r\",encoding='utf-8')\n",
    "    lines=file.readlines()\n",
    "    for l in lines:\n",
    "        tok_tweet=word_tokenize(clean_tweet(l.lower()))\n",
    "        bigrams=nltk.ngrams(tok_tweet,2)\n",
    "        score+=check(bigrams)\n",
    "    rw.write(str(i)+\":\"+str(score/len(lines))+\"\\n\")\n",
    "    \n",
    "fw.close()\n",
    "rw.close()\n",
    "'''\n",
    "fw=open(\"tweets.txt\",\"r\",encoding=\"utf-8\")\n",
    "rw=open(\"stan_tweet_result.txt\",\"a\",encoding='utf-8')\n",
    "tw=fw.read().split('\\n')\n",
    "i=0\n",
    "for t in tw:\n",
    "    i+=1\n",
    "    score=0.0\n",
    "    tok_tweet=word_tokenize(clean_tweet(t.lower()))\n",
    "    bigrams=nltk.ngrams(tok_tweet,2)\n",
    "    score=check(bigrams)\n",
    "    rw.write(str(t)+\"  :\"+str(score)+\"\\n\")\n",
    "    print(i)\n",
    "fw.close()\n",
    "rw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo=open(\"stan_tweet_result.txt\",\"r\",encoding='utf-8')\n",
    "lines=fo.readlines()\n",
    "s_list=[]\n",
    "for l in lines:\n",
    "    s_list.append(float(l.split(':')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list,neg_list=[],[]\n",
    "for s in s_list:\n",
    "    if s >=0.0:\n",
    "        pos_list.append(s)\n",
    "    if s<0.0:\n",
    "        neg_list.append(-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333333"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06727784728718064 : -0.01710215422679221\n",
      "0.0 : 0.0\n",
      "-0.625 : -0.3203478170731403\n",
      "0.125 : 0.08333333333333333\n",
      "0.5 : 0.3333333333333333\n",
      "0.25 : 0.16666666666666666\n",
      "0.0 : 0.0\n",
      "0.25 : 0.16666666666666666\n",
      "1.125 : 0.75\n",
      "0.0 : 0.0\n",
      "-1.25 : -0.6601739085365701\n",
      "0.08195822245770987 : 0.05463881497180658\n",
      "0.0 : 0.0\n",
      "-0.378 : -0.1860485457267928\n",
      "-0.5 : -0.2523825987804543\n",
      "-0.125 : -0.0484869439023964\n",
      "-0.25 : -0.11645216219508236\n",
      "0.5 : 0.3333333333333333\n",
      "-0.625 : -0.3203478170731403\n",
      "-0.5 : -0.2523825987804543\n",
      "-1.375 : -0.7281391268292561\n",
      "-0.625 : -0.3203478170731403\n",
      "1.0 : 0.6666666666666666\n",
      "0.375 : 0.25\n",
      "0.0 : 0.0\n",
      "-0.03582397526189089 : -0.0\n",
      "-0.25 : -0.11645216219508236\n",
      "0.75 : 0.5\n",
      "0.0 : 0.0\n",
      "-0.125 : -0.0484869439023964\n",
      "0.0 : 0.0\n",
      "-0.125 : -0.0484869439023964\n",
      "0.0 : 0.0\n",
      "1.125 : 0.75\n",
      "-0.75 : -0.3883130353658263\n",
      "-0.5 : -0.2523825987804543\n",
      "0.75 : 0.5\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "0.32422158219252517 : 0.21614772146168346\n",
      "0.0 : 0.0\n",
      "0.125 : 0.08333333333333333\n",
      "-1.375 : -0.7281391268292561\n",
      "-0.03582397526189089 : -0.0\n",
      "0.0 : 0.0\n",
      "0.32422158219252517 : 0.21614772146168346\n",
      "-0.25 : -0.11645216219508236\n",
      "-1.381 : -0.731401457307305\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-0.125 : -0.0484869439023964\n",
      "-0.25 : -0.11645216219508236\n",
      "0.375 : 0.25\n",
      "-0.25 : -0.11645216219508236\n",
      "0.125 : 0.08333333333333333\n",
      "0.0 : 0.0\n",
      "0.375 : 0.25\n",
      "-0.75 : -0.3883130353658263\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "0.625 : 0.4166666666666667\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-1.25 : -0.6601739085365701\n",
      "0.0 : 0.0\n",
      "0.25 : 0.16666666666666666\n",
      "0.875 : 0.5833333333333334\n",
      "-0.875 : -0.45627825365851227\n",
      "1.25 : 0.8333333333333334\n",
      "0.625 : 0.4166666666666667\n",
      "1.0 : 0.6666666666666666\n",
      "0.25 : 0.16666666666666666\n",
      "-0.06727784728718064 : -0.01710215422679221\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-0.625 : -0.3203478170731403\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "0.125 : 0.08333333333333333\n",
      "0.5 : 0.3333333333333333\n",
      "0.0 : 0.0\n",
      "-1.125 : -0.5922086902438841\n",
      "0.25 : 0.16666666666666666\n",
      "0.0 : 0.0\n",
      "0.75 : 0.5\n",
      "-0.625 : -0.3203478170731403\n",
      "-1.375 : -0.7281391268292561\n",
      "0.625 : 0.4166666666666667\n",
      "0.75 : 0.5\n",
      "-0.375 : -0.18441738048776835\n",
      "0.06054126524684952 : 0.04036084349789968\n",
      "0.125 : 0.08333333333333333\n",
      "0.05772215271281936 : 0.03848143514187957\n",
      "-1.375 : -0.7281391268292561\n",
      "0.0 : 0.0\n",
      "-0.75 : -0.3883130353658263\n",
      "0.75 : 0.5\n",
      "0.375 : 0.25\n",
      "0.5 : 0.3333333333333333\n",
      "0.32422158219252517 : 0.21614772146168346\n",
      "0.0 : 0.0\n",
      "-1.5 : -0.7961043451219421\n",
      "0.0 : 0.0\n",
      "-0.5 : -0.2523825987804543\n",
      "0.0 : 0.0\n",
      "-0.75 : -0.3883130353658263\n",
      "-0.375 : -0.18441738048776835\n",
      "-0.375 : -0.18441738048776835\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-0.375 : -0.18441738048776835\n",
      "-0.375 : -0.18441738048776835\n",
      "0.0 : 0.0\n",
      "-0.5 : -0.2523825987804543\n",
      "0.125 : 0.08333333333333333\n",
      "0.5 : 0.3333333333333333\n",
      "0.375 : 0.25\n",
      "0.0 : 0.0\n",
      "0.375 : 0.25\n",
      "0.0 : 0.0\n",
      "-0.875 : -0.45627825365851227\n",
      "0.375 : 0.25\n",
      "-0.378 : -0.1860485457267928\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-1.125 : -0.5922086902438841\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "0.625 : 0.4166666666666667\n",
      "0.625 : 0.4166666666666667\n",
      "-1.125 : -0.5922086902438841\n",
      "0.25 : 0.16666666666666666\n",
      "-1.875 : -1.0\n",
      "0.0 : 0.0\n",
      "0.375 : 0.25\n",
      "0.0 : 0.0\n",
      "-0.5 : -0.2523825987804543\n",
      "0.0 : 0.0\n",
      "-0.03582397526189089 : -0.0\n",
      "0.25 : 0.16666666666666666\n",
      "0.375 : 0.25\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-0.625 : -0.3203478170731403\n",
      "-0.25 : -0.11645216219508236\n",
      "-0.125 : -0.0484869439023964\n",
      "0.625 : 0.4166666666666667\n",
      "-0.375 : -0.18441738048776835\n",
      "-0.25 : -0.11645216219508236\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-1.375 : -0.7281391268292561\n",
      "0.0 : 0.0\n",
      "0.125 : 0.08333333333333333\n",
      "-0.125 : -0.0484869439023964\n",
      "0.75 : 0.5\n",
      "0.0 : 0.0\n",
      "0.875 : 0.5833333333333334\n",
      "0.0 : 0.0\n",
      "0.75 : 0.5\n",
      "0.375 : 0.25\n",
      "0.0 : 0.0\n",
      "-0.25 : -0.11645216219508236\n",
      "0.125 : 0.08333333333333333\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "0.25 : 0.16666666666666666\n",
      "-0.03582397526189089 : -0.0\n",
      "-1.0 : -0.5242434719511982\n",
      "0.0 : 0.0\n",
      "0.375 : 0.25\n",
      "-0.0358239752618908 : 4.9046757644439644e-17\n",
      "-0.375 : -0.18441738048776835\n",
      "0.5 : 0.3333333333333333\n",
      "-0.03582397526189089 : -0.0\n",
      "0.247 : 0.16466666666666666\n",
      "0.0 : 0.0\n",
      "0.0 : 0.0\n",
      "-0.125 : -0.0484869439023964\n",
      "0.0 : 0.0\n",
      "-0.625 : -0.3203478170731403\n",
      "-0.125 : -0.0484869439023964\n",
      "-0.125 : -0.0484869439023964\n",
      "0.125 : 0.08333333333333333\n",
      "-0.25 : -0.11645216219508236\n",
      "-0.625 : -0.3203478170731403\n",
      "0.625 : 0.4166666666666667\n",
      "-0.5 : -0.2523825987804543\n",
      "-0.25 : -0.11645216219508236\n"
     ]
    }
   ],
   "source": [
    "for s in s_list:\n",
    "    print(s,\":\",normalize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "word='loot'\n",
    "lemma = lemmatizer.lemmatize(word)            \n",
    "synsets = wn.synsets(lemma)\n",
    "synset = synsets[0]\n",
    "swn_synset = swn.senti_synset(synset.name())\n",
    "print(swn_synset.pos_score()) \n",
    "print(swn_synset.neg_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loot'"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loot'"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=synset.name()\n",
    "i=s.index('.')\n",
    "s[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word in senti_words:\n",
    "    line_nos=[index+1 for index, value in enumerate(senti_words) if value == word]\n",
    "    line_no=all_lines[senti_words.index(word)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "            \n",
    "\n",
    "elif lemma in senti_words: \n",
    "    line_no=lines[senti_words.index(lemma)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14272]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index+1 for index, value in enumerate(senti_words) if value == 'mad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a\\t02572038\\t0.375\\t0.25\\tmad#4 insane#2 harebrained#1\\tvery foolish; \"harebrained ideas\"; \"took insane risks behind the wheel\"; \"a completely mad scheme to build a bridge between two mountains\"\\n'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demonetization', 'NN'),\n",
       " ('+', 'NNP'),\n",
       " ('gst', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('biggest', 'JJS'),\n",
       " ('reset', 'NN'),\n",
       " ('indian', 'JJ'),\n",
       " ('economy', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('witnessed', 'VBN'),\n",
       " ('since', 'IN'),\n",
       " ('1991.', 'CD'),\n",
       " ('yes', 'NNS'),\n",
       " ('...', ':'),\n",
       " ('disruptive', 'NN'),\n",
       " (',', ','),\n",
       " ('chaotic', 'JJ'),\n",
       " ('but', 'CC'),\n",
       " ('was', 'VBD'),\n",
       " ('necessary', 'JJ')]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='''Demonetization + GST is the biggest reset Indian Economy has witnessed since 1991. Yes...disruptive, chaotic but was necessary '''\n",
    "a=word_tokenize(a.lower())\n",
    "\n",
    "pos_tag(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
