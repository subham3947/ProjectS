{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\tag\\stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "model='stanford-postagger-2018-02-27/models/english-left3words-distsim.tagger'\n",
    "jar='stanford-postagger-2018-02-27/stanford-postagger-3.9.1.jar'\n",
    "st=StanfordPOSTagger(model,jar,encoding='utf-8')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_words = []\n",
    "f=open(\"D:/Lex/SentiWordNet_3.0.0_20130122.txt\",\"r\",encoding=\"utf-8\") \n",
    "all_lines=f.readlines()\n",
    "for line in all_lines:\n",
    "    \n",
    "    w=line.split('\\t')[4]\n",
    "    #print(w)\n",
    "    i=w.index('#')\n",
    "    #print(i)\n",
    "    senti_words.append(w[:i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_words = []\n",
    "f=open(\"improvedsamplelexicon.txt\",\"r\",encoding=\"utf-8\") \n",
    "all_lines=f.readlines()\n",
    "for line in all_lines:\n",
    "    \n",
    "    w=line.split('\\t')[1]\n",
    "    senti_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_words.index('nightmare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg= {\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\",\"no\" \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\"unfulfilled\",\"undue\",\"anti\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost={\"absolutely\":0.5, \"amazingly\":0.125, \"awfully\":0.25, \"completely\":0.25, \"considerably\":0.125,\n",
    " \"decidedly\":0.25 , \"deeply\":0.0 , \"enormously\":0.25 ,\n",
    " \"entirely\":0.5 , \"especially\":0.0   , \"exceptionally\":0.0   , \"extremely\":0.625   ,\n",
    " \"fabulously\":0.25   , \"flipping\":0.0   , \"flippin\":0.0   ,\n",
    " \"fricking\":0.0   , \"frickin\":0.0   , \"frigging\":0.0   , \"friggin\":0.0   , \"fully\":0.375   , \"fucking\":0.125   ,\n",
    " \"greatly\":0.125   , \"hella\":0.0   , \"highly\":0.5   , \"hugely\":0.25   , \"incredibly\":0.25   ,\n",
    " \"intensely\":0.0   , \"majorly\":0.625   , \"more\":0.0  , \"most\":0.0   , \"particularly\":0.125   ,\n",
    " \"purely\":0.0   , \"quite\":0.125   , \"really\":0.375  , \"remarkably\":0.125   ,\n",
    " \"so\":0.0   , \"substantially\":0.125   ,\n",
    " \"thoroughly\":0.625   , \"totally\":0.5   , \"tremendously\":-0.25   ,\n",
    " \"uber\":0.0   , \"unbelievably\":0.25   , \"unusually\":0.0   , \"utterly\":0.5   ,\n",
    " \"very\":0.25   ,\"almost\":0.0   , \"barely\":-0.375   ,\"badly\":-0.75 ,\"hardly\":-0.25, \"kind of\":0.0   , \"kinda\":0.0 ,\n",
    " \"less\":-0.5   , \"little\":-0.375   , \"marginally\":-0.125   , \"occasionally\":0.0   , \"partly\":0.0   ,\n",
    " \"scarcely\":-0.25   , \"slightly\":-0.25   , \"somewhat\":-0.125, \"shoddy\":-0.625, \"poorly\":-0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(word,pos):\n",
    "    found=0\n",
    "    line_nos=[index for index, value in enumerate(senti_words) if value == word]\n",
    "    lines=[all_lines[lno] for lno in line_nos]\n",
    "    if len(lines)==0:\n",
    "        return None\n",
    "    for l in lines:\n",
    "        if l[0]==pos:\n",
    "            pos_score,neg_score=l.split('\\t')[2],l.split('\\t')[3]\n",
    "            found=1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    if found==1:\n",
    "        return(float(pos_score)-float(neg_score))\n",
    "    else:\n",
    "        pos_score,neg_score=lines[0].split('\\t')[2],lines[0].split('\\t')[3]\n",
    "        return(float(pos_score)-float(neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word,pos):\n",
    "    second_sent=find_sentiment(word,pos)\n",
    "    #print(word,second_sent)\n",
    "            \n",
    "    if second_sent is None:\n",
    "        try:\n",
    "            lemma=lemmatizer.lemmatize(word,pos)\n",
    "            synsets = wn.synsets(lemma)\n",
    "            synset=synsets[0]\n",
    "            s=synset.name()\n",
    "            i=s.index('.')\n",
    "            second_sent=find_sentiment(s[:i],pos)\n",
    "        except:\n",
    "            second_sent=0.0\n",
    "        #print(word,synset.name(),second_sent)\n",
    "    return second_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns=open('sent_nouns.txt','r',encoding='utf-8')\n",
    "nouns=nouns.read()\n",
    "nouns=nouns.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!@'#-+='''\n",
    "def clean_tweet(t):\n",
    "    for c in punctuations:\n",
    "             t= t.replace(c,\"\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1='''The marginalization of the Tax Cheats which was expected post #GST will now happen in right earnest as #Ewaybill rolls out. Real benefits to most organized sector companies will be seen in FY2019'''\n",
    "tweet2='''From Failed #Demonetisation to Shoddy Implementation of #GST to poorly performing #Economy to unfulfilled Promise of reduction in fuel prices to not keeping the promises of providing 2 crores jobs, BJP has wrecked a nation & has broken peopleâ€™s faith '''\n",
    "tweet3='''There is nothing wrong with #GST implementation but the fact is we are all losers. Injected to us by @INCIndia  Even God cannot change'''\n",
    "tweet4='''So, Indian businesses are mad against GST cause now they have to pay tax and earlier they weren't?'''\n",
    "tweet5='''#GST in india is total failure,it like nightmare for tax payers Indians..in my view it could be rolled out after #2019LSPolls after corrections ~ Dr Subramanian @Swamy39 Speaking at 14th Annual ðŸ‡®ðŸ‡³India Business Conference on â€˜Indian Growth @Columbia_Biz '''\n",
    "tweet6='''@rbi analysis is not accurate - #GST will not alter the aggregated capability of states to service debt.  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=word_tokenize(clean_tweet(tweet6.lower()))\n",
    "bigrams=nltk.ngrams(A,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rbi', 'NN'), ('analysis', 'NN')]\n",
      "[('analysis', 'NN'), ('is', 'VBZ')]\n",
      "[('is', 'VBZ'), ('not', 'RB')]\n",
      "[('not', 'RB'), ('accurate', 'JJ')]\n",
      "accurate 0.5\n",
      "Neg: [('not', 'RB'), ('accurate', 'JJ')] -0.5\n",
      "[('accurate', 'JJ'), ('gst', 'NN')]\n",
      "[('gst', 'NN'), ('will', 'MD')]\n",
      "[('will', 'MD'), ('not', 'RB')]\n",
      "[('not', 'RB'), ('alter', 'VB')]\n",
      "[('alter', 'VB'), ('the', 'DT')]\n",
      "[('the', 'DT'), ('aggregated', 'JJ')]\n",
      "Normal: [('the', 'DT'), ('aggregated', 'JJ')] -0.375\n",
      "[('aggregated', 'JJ'), ('capability', 'NN')]\n",
      "Normal: [('aggregated', 'JJ'), ('capability', 'NN')] -0.25\n",
      "[('capability', 'NN'), ('of', 'IN')]\n",
      "[('of', 'IN'), ('states', 'NNS')]\n",
      "[('states', 'NNS'), ('to', 'TO')]\n",
      "[('to', 'TO'), ('service', 'NN')]\n",
      "[('service', 'NN'), ('debt', 'NN')]\n",
      "Normal: [('service', 'NN'), ('debt', 'NN')] -0.25\n",
      "[('debt', 'NN'), ('.', '.')]\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "not_found=set()\n",
    "score=0.0\n",
    "for b in bigrams:\n",
    "    i+=1\n",
    "    b_tag=st.tag(b)\n",
    "    print(b_tag)\n",
    "    second_gram=b_tag[1]\n",
    "    first_gram=b_tag[0]\n",
    "    if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN') or second_gram[1].startswith('VBD'):\n",
    "    #if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VB'):\n",
    "        pos2=convert_tag(second_gram[1])\n",
    "        pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "        if first_gram[0] in boost:\n",
    "                score+=boost[first_gram[0]]+get_sentiment(second_gram[0],pos2)\n",
    "                print('Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "        if first_gram[0] in neg:\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    score+=(get_sentiment(first_gram[0],pos1)+sec_score)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                print('Neg:',b_tag,score)\n",
    "            \n",
    "        elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "            try:\n",
    "                #first_score=get_sentiment(first_gram[0],pos1)\n",
    "                score+=get_sentiment(second_gram[0],pos2)\n",
    "                print('Normal:',b_tag,score)\n",
    "            except:\n",
    "                print(\"Not Found:\",second_gram[0])\n",
    "                not_found.add(second_gram[0])\n",
    "    \n",
    "print(score)\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cant narendramodi govt spare Khadi,clean fabric & the symbol of India freedom struggle out of GST? '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweet('''Can't @narendramodi govt spare #Khadi,clean fabric & the symbol of India' freedom struggle out of #GST? ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(bigrams):\n",
    "    score=0.0\n",
    "    for b in bigrams:\n",
    "        b_tag=st.tag(b)\n",
    "        #b_tag=pos_tag(b)\n",
    "        #print(b_tag)\n",
    "        second_gram=b_tag[1]\n",
    "        first_gram=b_tag[0]\n",
    "        if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN'):\n",
    "            pos2=convert_tag(second_gram[1])\n",
    "            pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "            if first_gram[0] in boost:\n",
    "                    score+=boost[first_gram[0]]+get_sentiment(second_gram[0],pos2)\n",
    "                    #print('Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "            if first_gram[0] in neg:\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    try:\n",
    "                        x=get_sentiment(first_gram[0],pos1)\n",
    "                        score+=(x+sec_score)\n",
    "                    except:\n",
    "                        print(x)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                print('Neg:',b_tag,score)\n",
    "            \n",
    "            elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "                try:\n",
    "                    score+=get_sentiment(second_gram[0],pos2)\n",
    "                    #print('Normal:',b_tag,score)\n",
    "                except:\n",
    "                    print(\"Not Found:\",second_gram[0])\n",
    "                    #not_found.add(second_gram[0])\n",
    "    return score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "Not Found: drowned\n",
      "Not Found: showier\n",
      "17\n",
      "Not Found: 18th\n",
      "18\n",
      "19\n",
      "20\n",
      "Not Found: gripping\n",
      "21\n",
      "afford 0.0\n",
      "Neg: [('not', 'RB'), ('afford', 'VB')] -0.625\n",
      "22\n",
      "23\n",
      "Not Found: been\n",
      "24\n",
      "25\n",
      "26\n",
      "Not Found: drowned\n",
      "Not Found: showier\n",
      "27\n",
      "28\n",
      "Not Found: smiling\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "Not Found: looking\n",
      "45\n",
      "46\n",
      "47\n",
      "going 0.0\n",
      "Neg: [('not', 'RB'), ('going', 'VBG')] -0.625\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "Not Found: myth\n",
      "Not Found: predictable\n",
      "52\n",
      "53\n",
      "Not Found: witnessing\n",
      "54\n",
      "55\n",
      "Not Found: old\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "afford 0.0\n",
      "None\n",
      "Neg: [('cant', 'NN'), ('afford', 'VB')] 0.0\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "brought 0.0\n",
      "Neg: [('not', 'RB'), ('brought', 'VBN')] -0.625\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "Not Found: tiny\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "giving 0.0\n",
      "Neg: [('without', 'IN'), ('giving', 'VBG')] 0.0\n",
      "Not Found: anti\n",
      "Not Found: profiteering\n",
      "87\n",
      "advantage 0.625\n",
      "Neg: [('undue', 'JJ'), ('advantage', 'NN')] -0.75\n",
      "paying 0.0\n",
      "Neg: [('not', 'RB'), ('paying', 'VBG')] -1.375\n",
      "Not Found: profiteering\n",
      "88\n",
      "89\n",
      "90\n",
      "fair 0.375\n",
      "Neg: [('not', 'RB'), ('fair', 'JJ')] -0.375\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "Not Found: witnessed\n",
      "105\n",
      "106\n",
      "Not Found: poorest\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "Not Found: doing\n",
      "115\n",
      "116\n",
      "117\n",
      "Not Found: old\n",
      "118\n",
      "Not Found: asian\n",
      "119\n",
      "Not Found: nifty\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "applicable 0.5\n",
      "Neg: [('isnt', 'NN'), ('applicable', 'JJ')] -1.125\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "Not Found: strange\n",
      "141\n",
      "142\n",
      "143\n",
      "Not Found: reviewed\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "Not Found: old\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "Not Found: integral\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "Not Found: old\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "Not Found: been\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "Not Found: doing\n",
      "183\n",
      "184\n",
      "185\n",
      "Not Found: been\n",
      "186\n",
      "accurate 0.5\n",
      "Neg: [('not', 'RB'), ('accurate', 'JJ')] -0.5\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n"
     ]
    }
   ],
   "source": [
    "fw=open(\"tweets.txt\",\"r\",encoding=\"utf-8\")\n",
    "rw=open(\"tweet_result.txt\",\"a\",encoding='utf-8')\n",
    "tw=fw.read().split('\\n')\n",
    "i=0\n",
    "for t in tw:\n",
    "    i+=1\n",
    "    score=0.0\n",
    "    tok_tweet=word_tokenize(clean_tweet(t.lower()))\n",
    "    bigrams=nltk.ngrams(tok_tweet,2)\n",
    "    score=check(bigrams)\n",
    "    rw.write(str(t)+\"  :\"+str(score)+\"\\n\")\n",
    "    print(i)\n",
    "fw.close()\n",
    "rw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfile=open('not_found.txt','a',encoding='utf-8')\n",
    "for n in not_found:\n",
    "    wfile.write(n+\"\\n\")\n",
    "    i+=1\n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clarified'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "word='loot'\n",
    "lemma = lemmatizer.lemmatize(word)            \n",
    "synsets = wn.synsets(lemma)\n",
    "synset = synsets[0]\n",
    "swn_synset = swn.senti_synset(synset.name())\n",
    "print(swn_synset.pos_score()) \n",
    "print(swn_synset.neg_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loot'"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loot'"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=synset.name()\n",
    "i=s.index('.')\n",
    "s[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word in senti_words:\n",
    "    line_nos=[index+1 for index, value in enumerate(senti_words) if value == word]\n",
    "    line_no=all_lines[senti_words.index(word)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "            \n",
    "\n",
    "elif lemma in senti_words: \n",
    "    line_no=lines[senti_words.index(lemma)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14272]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index+1 for index, value in enumerate(senti_words) if value == 'mad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a\\t02572038\\t0.375\\t0.25\\tmad#4 insane#2 harebrained#1\\tvery foolish; \"harebrained ideas\"; \"took insane risks behind the wheel\"; \"a completely mad scheme to build a bridge between two mountains\"\\n'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demonetization', 'NN'),\n",
       " ('+', 'NNP'),\n",
       " ('gst', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('biggest', 'JJS'),\n",
       " ('reset', 'NN'),\n",
       " ('indian', 'JJ'),\n",
       " ('economy', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('witnessed', 'VBN'),\n",
       " ('since', 'IN'),\n",
       " ('1991.', 'CD'),\n",
       " ('yes', 'NNS'),\n",
       " ('...', ':'),\n",
       " ('disruptive', 'NN'),\n",
       " (',', ','),\n",
       " ('chaotic', 'JJ'),\n",
       " ('but', 'CC'),\n",
       " ('was', 'VBD'),\n",
       " ('necessary', 'JJ')]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='''Demonetization + GST is the biggest reset Indian Economy has witnessed since 1991. Yes...disruptive, chaotic but was necessary '''\n",
    "a=word_tokenize(a.lower())\n",
    "\n",
    "pos_tag(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
