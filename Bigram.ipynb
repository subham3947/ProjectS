{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\tag\\stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "model='stanford-postagger-2018-02-27/models/english-left3words-distsim.tagger'\n",
    "jar='stanford-postagger-2018-02-27/stanford-postagger-3.9.1.jar'\n",
    "st=StanfordPOSTagger(model,jar,encoding='utf-8')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_words = []\n",
    "f=open(\"D:/Lex/SentiWordNet_3.0.0_20130122.txt\",\"r\",encoding=\"utf-8\") \n",
    "all_lines=f.readlines()\n",
    "for line in all_lines:\n",
    "    \n",
    "    w=line.split('\\t')[4]\n",
    "    #print(w)\n",
    "    i=w.index('#')\n",
    "    #print(i)\n",
    "    senti_words.append(w[:i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_words = []\n",
    "f=open(\"improvedsamplelexicon.txt\",\"r\",encoding=\"utf-8\") \n",
    "all_lines=f.readlines()\n",
    "for line in all_lines:\n",
    "    \n",
    "    w=line.split('\\t')[1]\n",
    "    senti_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_words.index('nightmare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg= {\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\",\"no\" \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\"unfulfilled\",\"undue\",\"anti\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost={\"absolutely\":0.5, \"amazingly\":0.125, \"awfully\":0.25, \"completely\":0.25, \"considerably\":0.125,\n",
    " \"decidedly\":0.25 , \"deeply\":0.0 , \"enormously\":0.25 ,\n",
    " \"entirely\":0.5 , \"especially\":0.0   , \"exceptionally\":0.0   , \"extremely\":0.625   ,\n",
    " \"fabulously\":0.25   , \"flipping\":0.0   , \"flippin\":0.0   ,\n",
    " \"fricking\":0.0   , \"frickin\":0.0   , \"frigging\":0.0   , \"friggin\":0.0   , \"fully\":0.375   , \"fucking\":0.125   ,\n",
    " \"greatly\":0.125   , \"hella\":0.0   , \"highly\":0.5   , \"hugely\":0.25   , \"incredibly\":0.25   ,\n",
    " \"intensely\":0.0   , \"majorly\":0.625   , \"more\":0.0  , \"most\":0.0   , \"particularly\":0.125   ,\n",
    " \"purely\":0.0   , \"quite\":0.125   , \"really\":0.375  , \"remarkably\":0.125   ,\n",
    " \"so\":0.0   , \"substantially\":0.125   ,\n",
    " \"thoroughly\":0.625   , \"totally\":0.5   , \"tremendously\":-0.25   ,\n",
    " \"uber\":0.0   , \"unbelievably\":0.25   , \"unusually\":0.0   , \"utterly\":0.5   ,\n",
    " \"very\":0.25   ,\"almost\":0.0   , \"barely\":-0.375   ,\"badly\":-0.75 ,\"hardly\":-0.25, \"kind of\":0.0   , \"kinda\":0.0 ,\n",
    " \"less\":-0.5   , \"little\":-0.375   , \"marginally\":-0.125   , \"occasionally\":0.0   , \"partly\":0.0   ,\n",
    " \"scarcely\":-0.25   , \"slightly\":-0.25   , \"somewhat\":-0.125, \"shoddy\":-0.625, \"poorly\":-0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(word,pos):\n",
    "    found=0\n",
    "    line_nos=[index for index, value in enumerate(senti_words) if value == word]\n",
    "    lines=[all_lines[lno] for lno in line_nos]\n",
    "    if len(lines)==0:\n",
    "        return None\n",
    "    for l in lines:\n",
    "        if l[0]==pos:\n",
    "            pos_score,neg_score=l.split('\\t')[2],l.split('\\t')[3]\n",
    "            found=1\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    if found==1:\n",
    "        return(float(pos_score)-float(neg_score))\n",
    "    else:\n",
    "        pos_score,neg_score=lines[0].split('\\t')[2],lines[0].split('\\t')[3]\n",
    "        return(float(pos_score)-float(neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word,pos):\n",
    "    second_sent=find_sentiment(word,pos)\n",
    "    #print(word,second_sent)\n",
    "            \n",
    "    if second_sent is None:\n",
    "        try:\n",
    "            lemma=lemmatizer.lemmatize(word,pos)\n",
    "            synsets = wn.synsets(lemma)\n",
    "            synset=synsets[0]\n",
    "            s=synset.name()\n",
    "            i=s.index('.')\n",
    "            second_sent=find_sentiment(s[:i],pos)\n",
    "        except:\n",
    "            second_sent=0.0\n",
    "        #print(word,synset.name(),second_sent)\n",
    "    return second_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns=open('sent_nouns.txt','r',encoding='utf-8')\n",
    "nouns=nouns.read()\n",
    "nouns=nouns.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!@'#-+='''\n",
    "def clean_tweet(t):\n",
    "    for c in punctuations:\n",
    "             t= t.replace(c,\"\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1='''The marginalization of the Tax Cheats which was expected post #GST will now happen in right earnest as #Ewaybill rolls out. Real benefits to most organized sector companies will be seen in FY2019'''\n",
    "tweet2='''From Failed #Demonetisation to Shoddy Implementation of #GST to poorly performing #Economy to unfulfilled Promise of reduction in fuel prices to not keeping the promises of providing 2 crores jobs, BJP has wrecked a nation & has broken peopleâ€™s faith '''\n",
    "tweet3='''There is nothing wrong with #GST implementation but the fact is we are all losers. Injected to us by @INCIndia  Even God cannot change'''\n",
    "tweet4='''So, Indian businesses are mad against GST cause now they have to pay tax and earlier they weren't?'''\n",
    "tweet5='''#GST in india is total failure,it like nightmare for tax payers Indians..in my view it could be rolled out after #2019LSPolls after corrections ~ Dr Subramanian @Swamy39 Speaking at 14th Annual ðŸ‡®ðŸ‡³India Business Conference on â€˜Indian Growth @Columbia_Biz '''\n",
    "tweet6='''Now, GST leaves homestays baffled . '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=word_tokenize(clean_tweet(tweet6.lower()))\n",
    "bigrams=nltk.ngrams(A,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('now', 'RB'), (',', ',')]\n",
      "[(',', ','), ('gst', 'IN')]\n",
      "[('gst', 'NN'), ('leaves', 'VBZ')]\n",
      "[('leaves', 'VBZ'), ('homestays', 'NNS')]\n",
      "[('homestays', 'NNS'), ('baffled', 'VBD')]\n",
      "Normal: [('homestays', 'NNS'), ('baffled', 'VBD')] -0.25\n",
      "[('baffled', 'VBN'), ('.', '.')]\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "not_found=set()\n",
    "score=0.0\n",
    "for b in bigrams:\n",
    "    i+=1\n",
    "    b_tag=st.tag(b)\n",
    "    print(b_tag)\n",
    "    second_gram=b_tag[1]\n",
    "    first_gram=b_tag[0]\n",
    "    if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN') or second_gram[1].startswith('VBD'):\n",
    "    #if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VB'):\n",
    "        pos2=convert_tag(second_gram[1])\n",
    "        pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "        if first_gram[0] in boost:\n",
    "                score+=boost[first_gram[0]]+get_sentiment(second_gram[0],pos2)\n",
    "                print('Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "        if first_gram[0] in neg:\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    score+=(get_sentiment(first_gram[0],pos1)+sec_score)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                print('Neg:',b_tag,score)\n",
    "            \n",
    "        elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "            try:\n",
    "                #first_score=get_sentiment(first_gram[0],pos1)\n",
    "                score+=get_sentiment(second_gram[0],pos2)\n",
    "                print('Normal:',b_tag,score)\n",
    "            except:\n",
    "                print(\"Not Found:\",second_gram[0])\n",
    "                not_found.add(second_gram[0])\n",
    "    \n",
    "print(score)\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cant narendramodi govt spare Khadi,clean fabric & the symbol of India freedom struggle out of GST? '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweet('''Can't @narendramodi govt spare #Khadi,clean fabric & the symbol of India' freedom struggle out of #GST? ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(bigrams):\n",
    "    score=0.0\n",
    "    for b in bigrams:\n",
    "        b_tag=st.tag(b)\n",
    "        #b_tag=pos_tag(b)\n",
    "        print(b_tag)\n",
    "        second_gram=b_tag[1]\n",
    "        first_gram=b_tag[0]\n",
    "        if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN') or second_gram[1].startswith('VBD'):\n",
    "            pos2=convert_tag(second_gram[1])\n",
    "            pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "            if first_gram[0] in boost:\n",
    "                    score+=boost[first_gram[0]]+get_sentiment(second_gram[0],pos2)\n",
    "                    #print('Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "            if first_gram[0] in neg:\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    try:\n",
    "                        x=get_sentiment(first_gram[0],pos1)\n",
    "                        score+=(x+sec_score)\n",
    "                    except:\n",
    "                        print(x)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                print('Neg:',b_tag,score)\n",
    "            \n",
    "            elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "                try:\n",
    "                    score+=get_sentiment(second_gram[0],pos2)\n",
    "                    #print('Normal:',b_tag,score)\n",
    "                except:\n",
    "                    print(\"Not Found:\",second_gram[0])\n",
    "                    #not_found.add(second_gram[0])\n",
    "    return score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('now', 'RB'), (',', ',')]\n",
      "[(',', ','), ('gst', 'IN')]\n",
      "[('gst', 'NN'), ('leaves', 'VBZ')]\n",
      "[('leaves', 'VBZ'), ('homestays', 'NNS')]\n",
      "[('homestays', 'NNS'), ('baffled', 'VBD')]\n",
      "[('baffled', 'VBN'), ('.', '.')]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "fw=open(\"176tweets.txt\",\"r\",encoding=\"utf-8\")\n",
    "rw=open(\"stan_tweet_result.txt\",\"a\",encoding='utf-8')\n",
    "tw=fw.read().split('\\n')\n",
    "i=0\n",
    "for t in tw:\n",
    "    i+=1\n",
    "    score=0.0\n",
    "    tok_tweet=word_tokenize(clean_tweet(t.lower()))\n",
    "    bigrams=nltk.ngrams(tok_tweet,2)\n",
    "    score=check(bigrams)\n",
    "    rw.write(str(t)+\"  :\"+str(score)+\"\\n\")\n",
    "    print(i)\n",
    "fw.close()\n",
    "rw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "word='loot'\n",
    "lemma = lemmatizer.lemmatize(word)            \n",
    "synsets = wn.synsets(lemma)\n",
    "synset = synsets[0]\n",
    "swn_synset = swn.senti_synset(synset.name())\n",
    "print(swn_synset.pos_score()) \n",
    "print(swn_synset.neg_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loot'"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loot'"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=synset.name()\n",
    "i=s.index('.')\n",
    "s[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word in senti_words:\n",
    "    line_nos=[index+1 for index, value in enumerate(senti_words) if value == word]\n",
    "    line_no=all_lines[senti_words.index(word)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "            \n",
    "\n",
    "elif lemma in senti_words: \n",
    "    line_no=lines[senti_words.index(lemma)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14272]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index+1 for index, value in enumerate(senti_words) if value == 'mad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a\\t02572038\\t0.375\\t0.25\\tmad#4 insane#2 harebrained#1\\tvery foolish; \"harebrained ideas\"; \"took insane risks behind the wheel\"; \"a completely mad scheme to build a bridge between two mountains\"\\n'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('demonetization', 'NN'),\n",
       " ('+', 'NNP'),\n",
       " ('gst', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('biggest', 'JJS'),\n",
       " ('reset', 'NN'),\n",
       " ('indian', 'JJ'),\n",
       " ('economy', 'NN'),\n",
       " ('has', 'VBZ'),\n",
       " ('witnessed', 'VBN'),\n",
       " ('since', 'IN'),\n",
       " ('1991.', 'CD'),\n",
       " ('yes', 'NNS'),\n",
       " ('...', ':'),\n",
       " ('disruptive', 'NN'),\n",
       " (',', ','),\n",
       " ('chaotic', 'JJ'),\n",
       " ('but', 'CC'),\n",
       " ('was', 'VBD'),\n",
       " ('necessary', 'JJ')]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='''Demonetization + GST is the biggest reset Indian Economy has witnessed since 1991. Yes...disruptive, chaotic but was necessary '''\n",
    "a=word_tokenize(a.lower())\n",
    "\n",
    "pos_tag(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
