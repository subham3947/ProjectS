{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_words = []\n",
    "f=open(\"D:/Lex/SentiWordNet_3.0.0_20130122.txt\",\"r\",encoding=\"utf-8\") \n",
    "all_lines=f.readlines()\n",
    "for line in all_lines:\n",
    "    \n",
    "    w=line.split('\\t')[4]\n",
    "    #print(w)\n",
    "    i=w.index('#')\n",
    "    #print(i)\n",
    "    senti_words.append(w[:i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18368"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_words.index('failure')+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'failure'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_words[57756]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg= {\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\",\"no\" \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",\"unfulfilled\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost={\"absolutely\":0.5, \"amazingly\":0.125, \"awfully\":0.25, \"completely\":0.25, \"considerably\":0.125,\n",
    " \"decidedly\":0.25 , \"deeply\":0.0 , \"enormously\":0.25 ,\n",
    " \"entirely\":0.5 , \"especially\":0.0   , \"exceptionally\":0.0   , \"extremely\":0.625   ,\n",
    " \"fabulously\":0.25   , \"flipping\":0.0   , \"flippin\":0.0   ,\n",
    " \"fricking\":0.0   , \"frickin\":0.0   , \"frigging\":0.0   , \"friggin\":0.0   , \"fully\":0.375   , \"fucking\":0.125   ,\n",
    " \"greatly\":0.125   , \"hella\":0.0   , \"highly\":0.5   , \"hugely\":0.25   , \"incredibly\":0.25   ,\n",
    " \"intensely\":0.0   , \"majorly\":0.625   , \"more\":0.0  , \"most\":0.0   , \"particularly\":0.125   ,\n",
    " \"purely\":0.0   , \"quite\":0.125   , \"really\":0.375  , \"remarkably\":0.125   ,\n",
    " \"so\":0.0   , \"substantially\":0.125   ,\n",
    " \"thoroughly\":0.625   , \"totally\":0.5   , \"tremendously\":-0.25   ,\n",
    " \"uber\":0.0   , \"unbelievably\":0.25   , \"unusually\":0.0   , \"utterly\":0.5   ,\n",
    " \"very\":0.25   ,\n",
    " \"almost\":0.0   , \"barely\":-0.375   , \"hardly\":-0.25, \"kind of\":0.0   , \"kinda\":0.0   , \"kindof\":0.0   , \"kind-of\":0.0   ,\n",
    " \"less\":-0.5   , \"little\":-0.375   , \"marginally\":-0.125   , \"occasionally\":0.0   , \"partly\":0.0   ,\n",
    " \"scarcely\":-0.25   , \"slightly\":-0.25   , \"somewhat\":-0.125, \"shoddy\":-0.625,\"poorly\":-0.75}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(word,pos):\n",
    "    line_nos=[index for index, value in enumerate(senti_words) if value == word]\n",
    "    lines=[all_lines[lno] for lno in line_nos]\n",
    "    if len(lines)==0:\n",
    "        return None\n",
    "    for l in lines:\n",
    "        if l[0]==pos:\n",
    "            pos_score,neg_score=l.split('\\t')[2],l.split('\\t')[3]\n",
    "            return(float(pos_score)-float(neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word,pos):\n",
    "    second_sent=find_sentiment(word,pos)\n",
    "            \n",
    "    if second_sent is None:\n",
    "        lemma=lemmatizer.lemmatize(word,pos)\n",
    "        second_sent=find_sentiment(lemma,pos)\n",
    "    return second_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns=open('sent_nouns.txt','r',encoding='utf-8')\n",
    "nouns=nouns.read()\n",
    "nouns=nouns.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1='''The marginalization of the Tax Cheats which was expected post #GST will now happen in right earnest as #Ewaybill rolls out. Real benefits to most organized sector companies will be seen in FY2019'''\n",
    "tweet2='''From Failed #Demonetisation to Shoddy Implementation of #GST to poorly performing #Economy to unfulfilled Promise of reduction in fuel prices to not keeping the promises of providing 2 crores jobs, BJP has wrecked a nation & has broken peopleâ€™s faith '''\n",
    "tweet3='''There is nothing wrong with #GST implementation but the fact is we are all losers. Injected to us by @INCIndia  Even God cannot change'''\n",
    "tweet4='''So, Indian businesses are mad against GST cause now they have to pay tax and earlier they weren't?'''\n",
    "tweet5='''#GST in india is total failure,itâ€™s like nightmare for tax payers Indians..in my view it could be rolled out after #2019LSPolls after corrections ~ Dr Subramanian @Swamy39 Speaking at 14th Annual ðŸ‡®ðŸ‡³India Business Conference on â€˜Indian Growth @Columbia_Biz '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('D:/LEX/ProjectS-master/DATA/15Days/All_15.txt','r',encoding='utf-8')\n",
    "A=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=word_tokenize(tweet5.lower())\n",
    "bigrams=nltk.ngrams(A,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: [('is', 'VBZ'), ('total', 'JJ')] 0.0\n",
      "Normal: [('total', 'JJ'), ('failure', 'NN')] 0.125\n",
      "Normal: [('like', 'IN'), ('nightmare', 'NN')] -0.125\n",
      "Normal: [('be', 'VB'), ('rolled', 'VBN')] -0.125\n",
      "not found: annual\n",
      "Normal: [('â€˜', 'NN'), ('indian', 'JJ')] -0.125\n",
      "-0.125\n"
     ]
    }
   ],
   "source": [
    "score=0.0\n",
    "for b in bigrams:\n",
    "    b_tag=pos_tag(b)\n",
    "    #print(b_tag)\n",
    "    second_gram=b_tag[1]\n",
    "    first_gram=b_tag[0]\n",
    "    if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN'):\n",
    "        pos=convert_tag(second_gram[1])\n",
    "        \n",
    "        if first_gram[0] in boost:\n",
    "            score+=boost[first_gram[0]]+get_sentiment(second_gram[0],pos)\n",
    "            print('Boost:',b_tag,score)\n",
    "   \n",
    "        if first_gram[0] in neg:\n",
    "            score+=-(get_sentiment(second_gram[0],pos))\n",
    "            print('Neg:',b_tag,score)\n",
    "            \n",
    "        elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "            try:\n",
    "                score+=get_sentiment(second_gram[0],pos)\n",
    "                print('Normal:',b_tag,score)\n",
    "            except:\n",
    "                print('not found:',second_gram[0])\n",
    "print(score)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "word='mad'\n",
    "lemma = lemmatizer.lemmatize(word,pos='a')            \n",
    "synsets = wn.synsets(lemma,pos='a')\n",
    "synset = synsets[0]\n",
    "swn_synset = swn.senti_synset(synset.name())\n",
    "print(swn_synset.pos_score()) \n",
    "print(swn_synset.neg_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('huffy.s.02'),\n",
       " Synset('brainsick.s.01'),\n",
       " Synset('delirious.s.02'),\n",
       " Synset('harebrained.s.01')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word in senti_words:\n",
    "    line_nos=[index+1 for index, value in enumerate(senti_words) if value == word]\n",
    "    line_no=all_lines[senti_words.index(word)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "            \n",
    "\n",
    "elif lemma in senti_words: \n",
    "    line_no=lines[senti_words.index(lemma)]\n",
    "    pos_score,neg_score=line_no.split('\\t')[2],line_no.split('\\t')[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14272]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index+1 for index, value in enumerate(senti_words) if value == 'mad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a\\t02572038\\t0.375\\t0.25\\tmad#4 insane#2 harebrained#1\\tvery foolish; \"harebrained ideas\"; \"took insane risks behind the wheel\"; \"a completely mad scheme to build a bridge between two mountains\"\\n'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
