{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuations = '''!()-[]{};—:'\"\\,<>./’?@#$%^&*_~“”'''\n",
    "stop_words = stopwords.words('english') + list(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex_words=[]\n",
    "f = open(\"SentiLex.txt\",\"r\",encoding='utf-8') \n",
    "lines=f.readlines() \n",
    "for line in lines:\n",
    "    w=line.split('\\t')[1]\n",
    "    lex_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_words(text):\n",
    "    for c in punctuations:\n",
    "             text= text.replace(c,\" \")\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_sent(text):\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentiment=0.0\n",
    "    for s in sentences:\n",
    "        sentence_sentiment=0.0\n",
    "        w=sent_words(s)\n",
    "        tagged_words=pos_tag(w)\n",
    "        #print(tagged_words)\n",
    "        for word,tag in tagged_words:\n",
    "            wn_tag = convert_tag(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV,wn.VERB):\n",
    "                continue\n",
    "            lemma=lemmatizer.lemmatize(word,wn_tag)\n",
    "            if word in lex_words:\n",
    "                line_no=lines[lex_words.index(word)]\n",
    "                pos_score,neg_score=float(line_no.split('\\t')[2]),float(line_no.split('\\t')[3])\n",
    "            #or else if lemma in sentilex\n",
    "            \n",
    "            elif lemma in lex_words: \n",
    "                line_no=lines[lex_words.index(lemma)]\n",
    "                pos_score,neg_score=float(line_no.split('\\t')[2]),float(line_no.split('\\t')[3])\n",
    "                #print(word,'-',lemma)\n",
    "            #if not then skip\n",
    "            else:\n",
    "                continue\n",
    "            sentence_sentiment += pos_score - neg_score\n",
    "        sentiment+=sentence_sentiment\n",
    "    return sentiment/len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path=\"E:/ProjectS-master/Statesman_gst/July/Articles/\"\n",
    "l=os.listdir(path)\n",
    "wdfile=open('Statesman_July_Domain.txt','a',encoding='utf-8')\n",
    "wsfile=open('Statesman_July_SentiWordnet.txt','a',encoding='utf-8')\n",
    "for i in l:\n",
    "    try:\n",
    "        file=open(path+i,\"r\",encoding='utf-8')\n",
    "        article=file.read()\n",
    "        sent_score_domain_lexicon=calc_sent(article)\n",
    "        sent_score_sentiwordnet_lexicon=swn_sent(article)\n",
    "        wdfile.write(str(i.replace('.txt',''))+'\\t'+str(sent_score_domain_lexicon)+'\\n')\n",
    "        wsfile.write(str(i.replace('.txt',''))+'\\t'+str(sent_score_sentiwordnet_lexicon)+'\\n')\n",
    "    except:\n",
    "        print(i)\n",
    "wdfile.close()\n",
    "wsfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swn_sent(text):\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentiment=0.0\n",
    "    for s in sentences:\n",
    "        sentence_sentiment=0.0\n",
    "        w=sent_words(s)\n",
    "        tagged_words=pos_tag(w)\n",
    "        #print(tagged_words)\n",
    "        for word,tag in tagged_words:\n",
    "            wn_tag = convert_tag(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV,wn.VERB):\n",
    "                continue\n",
    "            lemma=lemmatizer.lemmatize(word,wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    " \n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    " \n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "            sentence_sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        sentiment+=sentence_sentiment\n",
    "    return sentiment/len(sentences)\n",
    "       \n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
