{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\nltk\\tag\\stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "model='stanford-postagger-2018-02-27/models/english-left3words-distsim.tagger'\n",
    "jar='stanford-postagger-2018-02-27/stanford-postagger-3.9.1.jar'\n",
    "st=StanfordPOSTagger(model,jar,encoding='utf-8')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "punctuations = '''!()-[]{};—:'\"\\,<>./’?@#$%^&*_~“”'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself'\n",
    ",'she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom',\n",
    " 'this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a',\n",
    " 'an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','between','into','through','during',\n",
    " 'before','after','to','from','again','then','once','here','there','when','where','why','how','all','any','both','each',\n",
    " 'than','should','d','ll','m','o','re','ve','y','ma']\n",
    "stopwords=stopwords+list(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg= {\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    " \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\",\"no\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\",\"minimise\", \"despite\",\"unfulfilled\",\"undue\",\"anti\",\"against\",\"without\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost={\"absolutely\":0.5, \"amazingly\":0.125, \"awfully\":0.25, \"completely\":0.25, \"considerably\":0.125,\n",
    " \"decidedly\":0.25 , \"deeply\":0.25 , \"enormously\":0.25 ,\n",
    " \"entirely\":0.5 , \"especially\":0.25 , \"exceptionally\":0.25, \"extremely\":0.625 ,\"fabulously\":0.25   , \"fully\":0.375  ,\n",
    " \"greatly\":0.125 , \"highly\":0.5   ,\"huge\":0.25, \"hugely\":0.25   , \"incredibly\":0.25   ,\n",
    " \"intensely\":0.25   , \"majorly\":0.625  ,\"particularly\":0.125 ,\"really\":0.375 ,\"substantially\":0.125   ,\n",
    " \"thoroughly\":0.625   , \"totally\":0.5  ,\"unbelievably\":0.25   , \"utterly\":0.5   ,\n",
    " \"very\":0.25  ,\"tremendously\":-0.25 ,  \"barely\":-0.375   ,\"badly\":-0.125 ,\"hardly\":-0.25,\"unusually\":-0.5   , \n",
    " \"less\":-0.5   , \"little\":-0.375   , \"marginally\":-0.125   , \"partly\":-0.125   ,\n",
    " \"scarcely\":-0.25   , \"slightly\":-0.25   , \"somewhat\":-0.125, \"shoddy\":-0.625, \"poorly\":-0.75}\n",
    "pos_boost=[\"absolutely\", \"amazingly\", \"awfully\", \"completely\", \"considerably\",\n",
    " \"decidedly\",\"deeply\",\"enormously\",\"entirely\",\"especially\",\"exceptionally\",\"extremely\",\"fabulously\",\"fully\",\n",
    " \"greatly\",\"highly\",\"huge\",\"hugely\", \"incredibly\",\"intensely\",\"majorly\",\"particularly\",\"really\",\"substantially\",\"thoroughly\",\"totally\",\"unbelievably\", \"utterly\",\"very\"]\n",
    "neg_boost=[\"tremendously\", \"barely\",\"badly\",\"hardly\",\"unusually\", \"less\", \"little\", \"marginally\", \"partly\",\n",
    " \"scarcely\", \"slightly\",\"somewhat\", \"shoddy\", \"poorly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!@'#-+=_'''\n",
    "def clean_tweet(t):\n",
    "    for c in punctuations:\n",
    "             t= t.replace(c,\" \")\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_booster(booster,word,pos2):\n",
    "    sec_score=get_sentiment(word,pos2)\n",
    "    if sec_score>0.0:\n",
    "        return(boost[booster]+sec_score)\n",
    "    if sec_score<0.0:\n",
    "        return(sec_score-boost[booster])\n",
    "    else:\n",
    "        return(boost[booster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_booster(booster,word,pos2):\n",
    "    sec_score=get_sentiment(word,pos2)\n",
    "    if sec_score>0.0:\n",
    "        return(sec_score+boost[booster])\n",
    "    if sec_score<0.0:\n",
    "        return(sec_score+boost[booster])\n",
    "    else:\n",
    "        return(boost[booster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(word,wn_tag):\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "    #print(word,synset,swn_synset.pos_score(),swn_synset.neg_score())\n",
    "    return(swn_synset.pos_score() - swn_synset.neg_score())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(score):\n",
    "    if score>=0.0:\n",
    "        return((score-(0.0))/2.75)\n",
    "    else:\n",
    "        return(-(-score-(0.125))/1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1='''The marginalization of the Tax Cheats which was expected post #GST will now happen in right earnest as #Ewaybill rolls out. Real benefits to most organized sector companies will be seen in FY2019'''\n",
    "tweet2='''From Failed #Demonetisation to Shoddy Implementation of #GST to poorly performing #Economy to unfulfilled Promise of reduction in fuel prices to not keeping the promises of providing 2 crores jobs, BJP has wrecked a nation & has broken people’s faith '''\n",
    "tweet3='''There is nothing wrong with #GST implementation but the fact is we are all losers. Injected to us by @INCIndia  Even God cannot change'''\n",
    "tweet4='''So, Indian businesses are mad against GST cause now they have to pay tax and earlier they weren't?'''\n",
    "tweet5='''#GST in india is total failure,it like nightmare for tax payers Indians..in my view it could be rolled out after #2019LSPolls after corrections ~ Dr Subramanian @Swamy39 Speaking at 14th Annual 🇮🇳India Business Conference on ‘Indian Growth @Columbia_Biz '''\n",
    "tweet6='''U know u r abt to witness #GST_Mess when a common citizen is forced to pay 28% #GST on BANANAS. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=word_tokenize(clean_tweet(tweet6.lower()))\n",
    "bigrams=nltk.ngrams(A,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found: mess\n",
      "a Synset('angstrom.n.01') 0.0 0.0\n",
      "common Synset('common.a.01') 0.0 0.0\n",
      "Normal: [('a', 'DT'), ('common', 'JJ')] 0.0\n",
      "is Synset('be.v.01') 0.25 0.125\n",
      "forced Synset('coerce.v.01') 0.375 0.125\n",
      "Normal: [('is', 'VBZ'), ('forced', 'VBN')] 0.375\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "not_found=set()\n",
    "score=0.0\n",
    "for b in bigrams:\n",
    "    i+=1\n",
    "    #b_tag=pos_tag(b)\n",
    "    b_tag=st.tag(b)\n",
    "    #print(b_tag)\n",
    "    second_gram=b_tag[1]\n",
    "    first_gram=b_tag[0]\n",
    "    if second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN') or second_gram[1].startswith('VBD'):\n",
    "    #if second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VB'):\n",
    "        pos2=convert_tag(second_gram[1])\n",
    "        pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "        if first_gram[0] in boost:\n",
    "            if first_gram[0] in pos_boost:\n",
    "                score=positive_booster(first_gram[0],second_gram[0],pos2)\n",
    "                print('Pos-Boost:',b_tag,score)\n",
    "            if first_gram[0] in neg_boost:\n",
    "                score=negative_booster(first_gram[0],second_gram[0],pos2)\n",
    "                print('Neg-Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "        if first_gram[0] in neg:\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    score+=(get_sentiment(first_gram[0],pos1)+sec_score)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                print('Neg:',b_tag,score)\n",
    "            \n",
    "        elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "            try:\n",
    "                first_score=get_sentiment(first_gram[0],pos1)\n",
    "                score+=first_score+get_sentiment(second_gram[0],pos2)\n",
    "                print('Normal:',b_tag,score)\n",
    "            except:\n",
    "                print(\"Not Found:\",second_gram[0])\n",
    "                not_found.add(second_gram[0])\n",
    "    \n",
    "print(score)\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(bigrams):\n",
    "    score=0.0\n",
    "    for b in bigrams:\n",
    "        b_tag=st.tag(b)\n",
    "        #b_tag=pos_tag(b)\n",
    "        #print(b_tag)\n",
    "        second_gram=b_tag[1]\n",
    "        first_gram=b_tag[0]\n",
    "        if second_gram[0] in nouns or second_gram[1].startswith('J') or second_gram[1].startswith('RBR') or second_gram[1].startswith('RBS') or second_gram[1].startswith('VBG') or second_gram[1].startswith('VBN') or second_gram[1].startswith('VBD'):\n",
    "            pos2=convert_tag(second_gram[1])\n",
    "            pos1=convert_tag(first_gram[1])\n",
    "        \n",
    "            if first_gram[0] in boost:\n",
    "                if first_gram[0] in pos_boost:\n",
    "                    score=positive_booster(first_gram[0],second_gram[0],pos2)\n",
    "                    print('Pos-Boost:',b_tag,score)\n",
    "                if first_gram[0] in neg_boost:\n",
    "                    score=negative_booster(first_gram[0],second_gram[0],pos2)\n",
    "                    print('Neg-Boost:',b_tag,score)\n",
    "            \n",
    "   \n",
    "            if first_gram[0] in neg:\n",
    "                x=0.0\n",
    "                sec_score=get_sentiment(second_gram[0],pos2)\n",
    "                print(second_gram[0],sec_score)\n",
    "                if sec_score ==0.0 or sec_score==None:\n",
    "                    try:\n",
    "                        x=get_sentiment(first_gram[0],pos1)\n",
    "                        score+=(x+sec_score)\n",
    "                    except:\n",
    "                        print(x)\n",
    "                else:\n",
    "                    score+=-(sec_score)\n",
    "                #print('Neg:',b_tag,score)\n",
    "            \n",
    "            elif first_gram[0] not in neg and first_gram[0] not in boost:\n",
    "                try:\n",
    "                    first_score=get_sentiment(first_gram[0],pos1)\n",
    "                    score+=first_score+get_sentiment(second_gram[0],pos2)\n",
    "                    #print('Normal:',b_tag,score)\n",
    "                except:\n",
    "                    print(\"Not Found:\",second_gram[0])\n",
    "                    #not_found.add(second_gram[0])\n",
    "    return (normalize(score))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found: expected\n",
      "Not Found: pain\n",
      "Not Found: indian\n",
      "Not Found: usual\n",
      "Not Found: promised\n",
      "Not Found: handling\n",
      "Not Found: economic\n",
      "doubt 0.125\n",
      "Not Found: more\n",
      "01-07-2017.txt\n",
      "Not Found: mess\n",
      "Not Found: depressing\n",
      "Not Found: most\n",
      "Not Found: indian\n",
      "Not Found: perfect\n",
      "Not Found: greater\n",
      "Not Found: permanent\n",
      "Not Found: boost\n",
      "Not Found: positive\n",
      "Not Found: expected\n",
      "Not Found: pain\n",
      "Not Found: indian\n",
      "02-07-2017.txt\n",
      "Not Found: faith\n",
      "Not Found: nuclear\n",
      "Not Found: proud\n",
      "Not Found: indian\n",
      "Not Found: supporting\n",
      "Not Found: little\n",
      "Neg-Boost: [('little', 'JJ'), ('demonetisation', 'NN')] -0.875\n",
      "Not Found: did\n",
      "Not Found: greatness\n",
      "Not Found: aborted\n",
      "more 0.0\n",
      "Not Found: biggest\n",
      "03-07-2017.txt\n",
      "Not Found: strange\n",
      "Not Found: multiple\n",
      "Not Found: .indian\n",
      "Not Found: central\n",
      "Not Found: similar\n",
      "Not Found: many\n",
      "Not Found: brilliance\n",
      "Not Found: many\n",
      "Not Found: struggling\n",
      "Not Found: new\n",
      "Not Found: seperate\n",
      "04-07-2017.txt\n",
      "Not Found: issues\n",
      "Pos-Boost: [('huge', 'JJ'), ('discomfort', 'NN')] -0.5\n",
      "Not Found: unable\n",
      "Not Found: demonetisation\n",
      "Not Found: implementing\n",
      "Not Found: led\n",
      "Not Found: historic\n",
      "Not Found: entire\n",
      "05-07-2017.txt\n",
      "Not Found: abolished\n",
      "Not Found: simple\n",
      "Not Found: implementing\n",
      "06-07-2017.txt\n",
      "Not Found: were\n",
      "Not Found: dropped\n",
      "Not Found: following\n",
      "Not Found: widespread\n",
      "Not Found: spare\n",
      "Not Found: clean\n",
      "07-07-2017.txt\n",
      "Not Found: been\n",
      "major 0.625\n",
      "Not Found: disaster\n",
      "Not Found: benefits\n",
      "Not Found: most\n",
      "Not Found: rich\n",
      "Not Found: rich\n",
      "Not Found: rich\n",
      "Not Found: common\n",
      "Not Found: added\n",
      "Not Found: given\n",
      "08-07-2017.txt\n",
      "Not Found: more\n",
      "Not Found: new\n",
      "Not Found: new\n",
      "Neg-Boost: [('little', 'RB'), ('hard', 'JJ')] -1.125\n",
      "Not Found: same\n",
      "Not Found: second\n",
      "Not Found: coming\n",
      "accurate 0.5\n",
      "Not Found: aggregated\n",
      "Not Found: new\n",
      "Not Found: starve\n",
      "Not Found: baffled\n",
      "09-07-2017.txt\n",
      "Neg-Boost: [('badly', 'RB'), ('affected', 'VBN')] 0.0\n",
      "Not Found: new\n",
      "10-07-2017.txt\n",
      "Not Found: previous\n",
      "Not Found: dominated\n",
      "Not Found: din\n",
      "11-07-2017.txt\n",
      "Not Found: offline\n",
      "Not Found: demonetization\n",
      "Not Found: difficult\n",
      "afford 0.0\n",
      "Pos-Boost: [('very', 'RB'), ('good', 'JJ')] 1.0\n",
      "Not Found: central\n",
      "Not Found: latest\n",
      "Not Found: protest\n",
      "Not Found: din\n",
      "12-07-2017.txt\n",
      "Not Found: central\n",
      "Not Found: gstsimplified\n",
      "Not Found: good\n",
      "Not Found: simple\n",
      "Not Found: liability\n",
      "Not Found: advantages\n",
      "Not Found: disadvantages\n",
      "Not Found: complexities\n",
      "Not Found: high\n",
      "Pos-Boost: [('really', 'RB'), ('killing', 'VBG')] -0.875\n",
      "Not Found: big\n",
      "Not Found: issues\n",
      "Not Found: included\n",
      "Not Found: addressed\n",
      "Not Found: concerns\n",
      "Not Found: demonetisation\n",
      "Not Found: low.industrial\n",
      "13-07-2017.txt\n",
      "14-07-2017.txt\n",
      "Not Found: more\n",
      "going 0.0\n",
      "Not Found: protest\n",
      "Not Found: useless\n",
      "15-07-2017.txt\n",
      "16-07-2017.txt\n",
      "Not Found: illegal\n",
      "Pos-Boost: [('completely', 'RB'), ('stopped', 'VBD')] 0.25\n",
      "Not Found: stable\n",
      "Not Found: hiking\n",
      "Not Found: other\n",
      "17-07-2017.txt\n",
      "Not Found: added\n",
      "Not Found: progress\n",
      "Not Found: warp\n",
      "Not Found: was\n",
      "18-07-2017.txt\n",
      "Not Found: good\n",
      "19-07-2017.txt\n",
      "brought 0.0\n",
      "Not Found: current\n",
      "Not Found: raised\n",
      "Not Found: were\n",
      "20-07-2017.txt\n",
      "Not Found: reduction\n",
      "Not Found: effective\n",
      "Not Found: tiny\n",
      "Not Found: abnormal\n",
      "21-07-2017.txt\n",
      "Not Found: organized\n",
      "giving -0.125\n",
      "fear -0.875\n",
      "Not Found: anti\n",
      "profiteering 0.0\n",
      "22-07-2017.txt\n",
      "advantage 0.625\n",
      "Not Found: were\n",
      "paying 0.0\n",
      "Not Found: profiteering\n",
      "Not Found: transparency\n",
      "23-07-2017.txt\n",
      "fair 0.625\n",
      "24-07-2017.txt\n",
      "Not Found: won\n",
      "Not Found: crack\n",
      "Not Found: more\n",
      "Not Found: small\n",
      "Not Found: unorganised\n",
      "Not Found: wholesale\n",
      "25-07-2017.txt\n",
      "Not Found: indian\n",
      "Not Found: ’\n",
      "Not Found: seated\n",
      "Not Found: retail\n",
      "Not Found: biggest\n",
      "Not Found: disruptive\n",
      "Not Found: chaotic\n",
      "26-07-2017.txt\n",
      "Not Found: poorest\n",
      "Not Found: burden\n",
      "Not Found: quicker\n",
      "Not Found: less\n",
      "Not Found: scrap\n",
      "Not Found: adverse\n",
      "Not Found: new\n",
      "Not Found: first\n",
      "Not Found: illegal\n",
      "Not Found: doing\n",
      "27-07-2017.txt\n",
      "Not Found: hail\n",
      "Not Found: same\n",
      "Not Found: common\n",
      "Not Found: other\n",
      "Not Found: touched\n",
      "Not Found: meeting\n",
      "Not Found: lauded\n",
      "28-07-2017.txt\n",
      "Not Found: exposed\n",
      "Not Found: low\n",
      "29-07-2017.txt\n",
      "Not Found: monthly\n",
      "Not Found: hike\n",
      "Not Found: same\n",
      "Not Found: last\n",
      "Not Found: much\n",
      "Not Found: most\n",
      "30-07-2017.txt\n",
      "hike 0.0\n",
      "Not Found: other\n",
      "Not Found: committed\n",
      "Not Found: exterminating\n",
      "Not Found: true\n",
      "Not Found: accusing\n",
      "Not Found: fiscal\n",
      "31-07-2017.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4af243306d49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mrw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m '''\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fw' is not defined"
     ]
    }
   ],
   "source": [
    "path=\"D:/LEX/ProjectS-master/tweet/\"\n",
    "tweet_list=os.listdir(path)\n",
    "#fw=open(\"tweets.txt\",\"r\",encoding=\"utf-8\")\n",
    "rw=open(\"sentiwordnet_normalized_day_tweet_result.txt\",\"a\",encoding='utf-8')\n",
    "#tw=fw.read().split('\\n')\n",
    "for i in tweet_list:\n",
    "    score=0.0\n",
    "    file=open(path+i,\"r\",encoding='utf-8')\n",
    "    lines=file.readlines()\n",
    "    for l in lines:\n",
    "        tok_tweet=word_tokenize(clean_tweet(l.lower()))\n",
    "        bigrams=nltk.ngrams(tok_tweet,2)\n",
    "        score+=check(bigrams)\n",
    "    rw.write(str(i)+\":\"+str(score/len(lines))+\"\\n\")\n",
    "    print(i)\n",
    "rw.close()\n",
    "'''\n",
    "fw=open(\"tweets.txt\",\"r\",encoding=\"utf-8\")\n",
    "rw=open(\"norm_stan_tweet_result.txt\",\"a\",encoding='utf-8')\n",
    "tw=fw.read().split('\\n')\n",
    "i=0\n",
    "for t in tw:\n",
    "    i+=1\n",
    "    score=0.0\n",
    "    tok_tweet=word_tokenize(clean_tweet(t.lower()))\n",
    "    bigrams=nltk.ngrams(tok_tweet,2)\n",
    "    score=check(bigrams)\n",
    "    rw.write(str(t)+\"  :\"+str(score)+\"\\n\")\n",
    "    print(i)\n",
    "fw.close()\n",
    "rw.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo=open(\"Result/Tweets/SentiWordnet/Sentiwordnet_stan_tweet_result.txt\",\"r\",encoding='utf-8')\n",
    "lines=fo.readlines()\n",
    "s_list=[]\n",
    "for l in lines:\n",
    "    #print(l)\n",
    "    s_list.append(float(l.split(':')[1]))\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list,neg_list=[],[]\n",
    "for s in s_list:\n",
    "    if s >=0.0:\n",
    "        pos_list.append(s)\n",
    "    if s<0.0:\n",
    "        neg_list.append(-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.375"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.375-0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
